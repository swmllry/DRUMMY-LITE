{"ast":null,"code":"\"use strict\";\n\nObject.defineProperty(exports, \"__esModule\", {\n  value: true\n});\nexports.OrderedBulkOperation = void 0;\n\nconst BSON = require(\"../bson\");\n\nconst error_1 = require(\"../error\");\n\nconst common_1 = require(\"./common\");\n/** @public */\n\n\nclass OrderedBulkOperation extends common_1.BulkOperationBase {\n  constructor(collection, options) {\n    super(collection, options, true);\n  }\n\n  addToOperationsList(batchType, document) {\n    // Get the bsonSize\n    const bsonSize = BSON.calculateObjectSize(document, {\n      checkKeys: false,\n      // Since we don't know what the user selected for BSON options here,\n      // err on the safe side, and check the size with ignoreUndefined: false.\n      ignoreUndefined: false\n    }); // Throw error if the doc is bigger than the max BSON size\n\n    if (bsonSize >= this.s.maxBsonObjectSize) // TODO(NODE-3483): Change this to MongoBSONError\n      throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`); // Create a new batch object if we don't have a current one\n\n    if (this.s.currentBatch == null) {\n      this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n    }\n\n    const maxKeySize = this.s.maxKeySize; // Check if we need to create a new batch\n\n    if ( // New batch if we exceed the max batch op size\n    this.s.currentBatchSize + 1 >= this.s.maxWriteBatchSize || // New batch if we exceed the maxBatchSizeBytes. Only matters if batch already has a doc,\n    // since we can't sent an empty batch\n    this.s.currentBatchSize > 0 && this.s.currentBatchSizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes || // New batch if the new op does not have the same op type as the current batch\n    this.s.currentBatch.batchType !== batchType) {\n      // Save the batch to the execution stack\n      this.s.batches.push(this.s.currentBatch); // Create a new batch\n\n      this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex); // Reset the current size trackers\n\n      this.s.currentBatchSize = 0;\n      this.s.currentBatchSizeBytes = 0;\n    }\n\n    if (batchType === common_1.BatchType.INSERT) {\n      this.s.bulkResult.insertedIds.push({\n        index: this.s.currentIndex,\n        _id: document._id\n      });\n    } // We have an array of documents\n\n\n    if (Array.isArray(document)) {\n      throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');\n    }\n\n    this.s.currentBatch.originalIndexes.push(this.s.currentIndex);\n    this.s.currentBatch.operations.push(document);\n    this.s.currentBatchSize += 1;\n    this.s.currentBatchSizeBytes += maxKeySize + bsonSize;\n    this.s.currentIndex += 1;\n    return this;\n  }\n\n}\n\nexports.OrderedBulkOperation = OrderedBulkOperation;","map":{"version":3,"sources":["../../src/bulk/ordered.ts"],"names":[],"mappings":";;;;;;;AACA,MAAA,IAAA,GAAA,OAAA,CAAA,SAAA,CAAA;;AAEA,MAAA,OAAA,GAAA,OAAA,CAAA,UAAA,CAAA;;AAGA,MAAA,QAAA,GAAA,OAAA,CAAA,UAAA,CAAA;AAEA;;;AACA,MAAa,oBAAb,SAA0C,QAAA,CAAA,iBAA1C,CAA2D;EACzD,WAAA,CAAY,UAAZ,EAAoC,OAApC,EAA6D;IAC3D,MAAM,UAAN,EAAkB,OAAlB,EAA2B,IAA3B;EACD;;EAED,mBAAmB,CACjB,SADiB,EAEjB,QAFiB,EAEqC;IAEtD;IACA,MAAM,QAAQ,GAAG,IAAI,CAAC,mBAAL,CAAyB,QAAzB,EAAmC;MAClD,SAAS,EAAE,KADuC;MAElD;MACA;MACA,eAAe,EAAE;IAJiC,CAAnC,CAAjB,CAHsD,CAUtD;;IACA,IAAI,QAAQ,IAAI,KAAK,CAAL,CAAO,iBAAvB,EACE;MACA,MAAM,IAAI,OAAA,CAAA,yBAAJ,CACJ,4CAA4C,KAAK,CAAL,CAAO,iBAAiB,EADhE,CAAN,CAboD,CAiBtD;;IACA,IAAI,KAAK,CAAL,CAAO,YAAP,IAAuB,IAA3B,EAAiC;MAC/B,KAAK,CAAL,CAAO,YAAP,GAAsB,IAAI,QAAA,CAAA,KAAJ,CAAU,SAAV,EAAqB,KAAK,CAAL,CAAO,YAA5B,CAAtB;IACD;;IAED,MAAM,UAAU,GAAG,KAAK,CAAL,CAAO,UAA1B,CAtBsD,CAwBtD;;IACA,KACE;IACA,KAAK,CAAL,CAAO,gBAAP,GAA0B,CAA1B,IAA+B,KAAK,CAAL,CAAO,iBAAtC,IACA;IACA;IACC,KAAK,CAAL,CAAO,gBAAP,GAA0B,CAA1B,IACC,KAAK,CAAL,CAAO,qBAAP,GAA+B,UAA/B,GAA4C,QAA5C,IAAwD,KAAK,CAAL,CAAO,iBAJjE,IAKA;IACA,KAAK,CAAL,CAAO,YAAP,CAAoB,SAApB,KAAkC,SARpC,EASE;MACA;MACA,KAAK,CAAL,CAAO,OAAP,CAAe,IAAf,CAAoB,KAAK,CAAL,CAAO,YAA3B,EAFA,CAIA;;MACA,KAAK,CAAL,CAAO,YAAP,GAAsB,IAAI,QAAA,CAAA,KAAJ,CAAU,SAAV,EAAqB,KAAK,CAAL,CAAO,YAA5B,CAAtB,CALA,CAOA;;MACA,KAAK,CAAL,CAAO,gBAAP,GAA0B,CAA1B;MACA,KAAK,CAAL,CAAO,qBAAP,GAA+B,CAA/B;IACD;;IAED,IAAI,SAAS,KAAK,QAAA,CAAA,SAAA,CAAU,MAA5B,EAAoC;MAClC,KAAK,CAAL,CAAO,UAAP,CAAkB,WAAlB,CAA8B,IAA9B,CAAmC;QACjC,KAAK,EAAE,KAAK,CAAL,CAAO,YADmB;QAEjC,GAAG,EAAG,QAAqB,CAAC;MAFK,CAAnC;IAID,CAnDqD,CAqDtD;;;IACA,IAAI,KAAK,CAAC,OAAN,CAAc,QAAd,CAAJ,EAA6B;MAC3B,MAAM,IAAI,OAAA,CAAA,yBAAJ,CAA8B,wCAA9B,CAAN;IACD;;IAED,KAAK,CAAL,CAAO,YAAP,CAAoB,eAApB,CAAoC,IAApC,CAAyC,KAAK,CAAL,CAAO,YAAhD;IACA,KAAK,CAAL,CAAO,YAAP,CAAoB,UAApB,CAA+B,IAA/B,CAAoC,QAApC;IACA,KAAK,CAAL,CAAO,gBAAP,IAA2B,CAA3B;IACA,KAAK,CAAL,CAAO,qBAAP,IAAgC,UAAU,GAAG,QAA7C;IACA,KAAK,CAAL,CAAO,YAAP,IAAuB,CAAvB;IACA,OAAO,IAAP;EACD;;AAvEwD;;AAA3D,OAAA,CAAA,oBAAA,GAAA,oBAAA","sourceRoot":"","sourcesContent":["\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.OrderedBulkOperation = void 0;\nconst BSON = require(\"../bson\");\nconst error_1 = require(\"../error\");\nconst common_1 = require(\"./common\");\n/** @public */\nclass OrderedBulkOperation extends common_1.BulkOperationBase {\n    constructor(collection, options) {\n        super(collection, options, true);\n    }\n    addToOperationsList(batchType, document) {\n        // Get the bsonSize\n        const bsonSize = BSON.calculateObjectSize(document, {\n            checkKeys: false,\n            // Since we don't know what the user selected for BSON options here,\n            // err on the safe side, and check the size with ignoreUndefined: false.\n            ignoreUndefined: false\n        });\n        // Throw error if the doc is bigger than the max BSON size\n        if (bsonSize >= this.s.maxBsonObjectSize)\n            // TODO(NODE-3483): Change this to MongoBSONError\n            throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);\n        // Create a new batch object if we don't have a current one\n        if (this.s.currentBatch == null) {\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n        }\n        const maxKeySize = this.s.maxKeySize;\n        // Check if we need to create a new batch\n        if (\n        // New batch if we exceed the max batch op size\n        this.s.currentBatchSize + 1 >= this.s.maxWriteBatchSize ||\n            // New batch if we exceed the maxBatchSizeBytes. Only matters if batch already has a doc,\n            // since we can't sent an empty batch\n            (this.s.currentBatchSize > 0 &&\n                this.s.currentBatchSizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes) ||\n            // New batch if the new op does not have the same op type as the current batch\n            this.s.currentBatch.batchType !== batchType) {\n            // Save the batch to the execution stack\n            this.s.batches.push(this.s.currentBatch);\n            // Create a new batch\n            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);\n            // Reset the current size trackers\n            this.s.currentBatchSize = 0;\n            this.s.currentBatchSizeBytes = 0;\n        }\n        if (batchType === common_1.BatchType.INSERT) {\n            this.s.bulkResult.insertedIds.push({\n                index: this.s.currentIndex,\n                _id: document._id\n            });\n        }\n        // We have an array of documents\n        if (Array.isArray(document)) {\n            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');\n        }\n        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);\n        this.s.currentBatch.operations.push(document);\n        this.s.currentBatchSize += 1;\n        this.s.currentBatchSizeBytes += maxKeySize + bsonSize;\n        this.s.currentIndex += 1;\n        return this;\n    }\n}\nexports.OrderedBulkOperation = OrderedBulkOperation;\n//# sourceMappingURL=ordered.js.map"]},"metadata":{},"sourceType":"script"}